{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d0418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aww66/.conda/envs/huggingface/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "# For machine learning tools and evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Transformer library\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7a1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"Judaism\"\n",
    "usecols=['year-month', 'timestamp', 'text', 'speaker']\n",
    "comments_df = pk.load(open(data_dir + f\"{subreddit}-comments.pk\", \"rb\"))\n",
    "comments_df = comments_df[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aaeec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_month = \"2016-01\"\n",
    "checkpoint_path = f\"./models/distilgpt2_{subreddit}_{model_month}/best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a2649fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91484a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8669"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_month = \"2017-04\"\n",
    "input_texts = [t for t in comments_df[comments_df['year-month'] == predict_month]['text'] if t]\n",
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc2e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_random_utts = np.random.choice(input_texts, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3de4034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_random_utts = [preprocess(u) for u in selected_random_utts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b38fe1",
   "metadata": {},
   "source": [
    "## use evaluate library to calculate sentence perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e129e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_huggingface_ppl(sentences):\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    return perplexity.compute(\n",
    "        model_id = checkpoint_path,\n",
    "        add_start_token = False, # default\n",
    "        predictions = sentences,\n",
    "        max_length=1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a846654d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 10.34it/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_ppl = calculate_huggingface_ppl(selected_random_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f7ebe363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexities': [119.88584899902344,\n",
       "  236.0976104736328,\n",
       "  100.75423431396484,\n",
       "  716.7677001953125,\n",
       "  413.7164306640625,\n",
       "  410.6705322265625,\n",
       "  248.873291015625,\n",
       "  163.5489959716797,\n",
       "  55.92713928222656,\n",
       "  160.4458770751953,\n",
       "  153.39732360839844,\n",
       "  104.64350891113281,\n",
       "  92.46155548095703,\n",
       "  96.78690338134766,\n",
       "  387.2434387207031,\n",
       "  190.98699951171875,\n",
       "  470.62890625,\n",
       "  185.51380920410156,\n",
       "  147.025390625,\n",
       "  653.4151000976562,\n",
       "  265.0249328613281,\n",
       "  139.4586944580078,\n",
       "  198.70712280273438,\n",
       "  94.5572280883789,\n",
       "  800.6076049804688,\n",
       "  455.3201599121094,\n",
       "  19.6213321685791,\n",
       "  91.93151092529297,\n",
       "  84.49825286865234,\n",
       "  427.1061706542969,\n",
       "  742.717529296875,\n",
       "  223.286865234375,\n",
       "  53.72596740722656,\n",
       "  58.15818405151367,\n",
       "  100.2213134765625,\n",
       "  235.43734741210938,\n",
       "  141.0738525390625,\n",
       "  48.792850494384766,\n",
       "  35.06508255004883,\n",
       "  257.86676025390625,\n",
       "  291.4250183105469,\n",
       "  394.947021484375,\n",
       "  107.3738021850586,\n",
       "  63.94613265991211,\n",
       "  124.24491119384766,\n",
       "  110.55298614501953,\n",
       "  679.3197631835938,\n",
       "  373.8709411621094,\n",
       "  80.6605224609375,\n",
       "  173.92201232910156,\n",
       "  251.0616912841797,\n",
       "  636.2868041992188,\n",
       "  56.44964599609375,\n",
       "  85.11417388916016,\n",
       "  128.07455444335938,\n",
       "  138.98338317871094,\n",
       "  71.83094024658203,\n",
       "  559.5365600585938,\n",
       "  340.7301940917969,\n",
       "  606.6831665039062,\n",
       "  168.40550231933594,\n",
       "  127.5880126953125,\n",
       "  97.48611450195312,\n",
       "  146.85107421875,\n",
       "  159.1952362060547,\n",
       "  178.58255004882812,\n",
       "  66.19402313232422,\n",
       "  485.7581787109375,\n",
       "  859.8264770507812,\n",
       "  293.88189697265625,\n",
       "  381.3898010253906,\n",
       "  79.14262390136719,\n",
       "  45.56159210205078,\n",
       "  103.61669158935547,\n",
       "  155.8438262939453,\n",
       "  166.87725830078125,\n",
       "  91.4820556640625,\n",
       "  482.5110168457031,\n",
       "  138.9982147216797,\n",
       "  140.3814697265625,\n",
       "  105.6721420288086,\n",
       "  68.22650909423828,\n",
       "  160.16656494140625,\n",
       "  170.22557067871094,\n",
       "  167.33383178710938,\n",
       "  151.22283935546875,\n",
       "  907.9901123046875,\n",
       "  99.54983520507812,\n",
       "  191.15081787109375,\n",
       "  288.0614013671875,\n",
       "  34.04652404785156,\n",
       "  85.326904296875,\n",
       "  267.1112365722656,\n",
       "  194.7720947265625,\n",
       "  2492.619140625,\n",
       "  77.8310775756836,\n",
       "  44.59421157836914,\n",
       "  87.70103454589844,\n",
       "  2539.57177734375,\n",
       "  87.12283325195312],\n",
       " 'mean_perplexity': 274.4685365867615}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f012f",
   "metadata": {},
   "source": [
    "## modified huggingface implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "76ddfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import evaluate\n",
    "from evaluate import logging\n",
    "\n",
    "_DESCRIPTION = \"\"\n",
    "_CITATION = \"\"\n",
    "_KWARGS_DESCRIPTION = \"\"\n",
    "class mod_Perplexity(evaluate.Measurement):\n",
    "    def _info(self):\n",
    "        return evaluate.MeasurementInfo(\n",
    "            module_type=\"measurement\",\n",
    "            description=_DESCRIPTION,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"data\": datasets.Value(\"string\"),\n",
    "                }\n",
    "            ),\n",
    "            reference_urls=[\"https://huggingface.co/docs/transformers/perplexity\"],\n",
    "        )\n",
    "\n",
    "    def _compute(\n",
    "        self, data, model_id, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "    ):\n",
    "\n",
    "        if device is not None:\n",
    "            assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "            if device == \"gpu\":\n",
    "                device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        model = model.to(device)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        # if batch_size > 1 (which generally leads to padding being required), and\n",
    "        # if there is not an already assigned pad_token, assign an existing\n",
    "        # special token to also be the padding token\n",
    "        if tokenizer.pad_token is None and batch_size > 1:\n",
    "            existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "            # check that the model already has at least one special token defined\n",
    "            assert (\n",
    "                len(existing_special_tokens) > 0\n",
    "            ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "            # assign one of the special tokens to also be the pad token\n",
    "            tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "        if add_start_token and max_length:\n",
    "            # leave room for <BOS> token to be added:\n",
    "            assert (\n",
    "                tokenizer.bos_token is not None\n",
    "            ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "            max_tokenized_len = max_length - 1\n",
    "        else:\n",
    "            max_tokenized_len = max_length\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            data,\n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True if max_tokenized_len else False,\n",
    "            max_length=max_tokenized_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        encoded_texts = encodings[\"input_ids\"]\n",
    "        attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "        # check that each input is long enough:\n",
    "        if add_start_token:\n",
    "            assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "        else:\n",
    "            assert torch.all(\n",
    "                torch.ge(attn_masks.sum(1), 2)\n",
    "            ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "        sentence_ppls = []\n",
    "        token_ppls = []\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        for start_index in logging.tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "            end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "            encoded_batch = encoded_texts[start_index:end_index]\n",
    "            attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "            if add_start_token:\n",
    "                bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "                encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "                attn_mask = torch.cat(\n",
    "                    [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "                )\n",
    "\n",
    "            labels = encoded_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "            \n",
    "            ## the following parts are different from the original code\n",
    "\n",
    "            token_CEloss = loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch\n",
    "    \n",
    "            ppl_by_token = torch.exp(torch.div(token_CEloss.T, shift_attention_mask_batch.sum(1))).T\n",
    "\n",
    "            batch_token_ppls = [tuple(zip(encoded_batch.cpu().numpy()[i], ppl_by_token.cpu().numpy()[i])) for i in range(encoded_batch.shape[0])]\n",
    "\n",
    "            perplexity_batch = torch.prod(ppl_by_token, 1)\n",
    "\n",
    "            sentence_ppls += perplexity_batch.tolist()\n",
    "            token_ppls += batch_token_ppls\n",
    "        return {\"sentence_ppls\": sentence_ppls, \"token_ppls\": token_ppls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c2e5a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(selected_random_utts)\n",
    "model_id = checkpoint_path\n",
    "max_length = 1024\n",
    "add_start_token = False\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f1657b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 10.15it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_ppl = mod_Perplexity()\n",
    "stash_ppl = mod_ppl._compute(\n",
    "    data = data,\n",
    "    model_id = model_id,\n",
    "    add_start_token = False, # default,\n",
    "    max_length = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e430cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "2539.572265625\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(stash_ppl['sentence_ppls'])\n",
    "print(idx)\n",
    "print(stash_ppl['sentence_ppls'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fe592278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10378, 4.1920857),\n",
       " (2412, 2.1464508),\n",
       " (319, 4.0804687),\n",
       " (3025, 9.030929),\n",
       " (16511, 1.462644),\n",
       " (339, 5.2363467),\n",
       " (3951, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0),\n",
       " (50256, 1.0))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stash_ppl['token_ppls'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "caca00c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'depends on whose pockets he lines'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_random_utts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6ae8018d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 10.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# if batch_size > 1 (which generally leads to padding being required), and\n",
    "# if there is not an already assigned pad_token, assign an existing\n",
    "# special token to also be the padding token\n",
    "if tokenizer.pad_token is None and batch_size > 1:\n",
    "    existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "    # check that the model already has at least one special token defined\n",
    "    assert (\n",
    "        len(existing_special_tokens) > 0\n",
    "    ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "    # assign one of the special tokens to also be the pad token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "if add_start_token and max_length:\n",
    "    # leave room for <BOS> token to be added:\n",
    "    assert (\n",
    "        tokenizer.bos_token is not None\n",
    "    ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "    max_tokenized_len = max_length - 1\n",
    "else:\n",
    "    max_tokenized_len = max_length\n",
    "\n",
    "encodings = tokenizer(\n",
    "    data,\n",
    "    add_special_tokens=False,\n",
    "    padding=True,\n",
    "    truncation=True if max_tokenized_len else False,\n",
    "    max_length=max_tokenized_len,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(device)\n",
    "\n",
    "encoded_texts = encodings[\"input_ids\"]\n",
    "attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "# check that each input is long enough:\n",
    "if add_start_token:\n",
    "    assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "else:\n",
    "    assert torch.all(\n",
    "        torch.ge(attn_masks.sum(1), 2)\n",
    "    ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "ppls = []\n",
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "for start_index in logging.tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "    end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "    encoded_batch = encoded_texts[start_index:end_index]\n",
    "    attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "    if add_start_token:\n",
    "        bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "        encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "        attn_mask = torch.cat(\n",
    "            [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "        )\n",
    "\n",
    "    labels = encoded_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "    shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "    \n",
    "    perplexity_batch = torch.exp(\n",
    "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                / shift_attention_mask_batch.sum(1)\n",
    "            )\n",
    "\n",
    "    ppls += perplexity_batch.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "542c3b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4361262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import calc_token_contribution\n",
    "# import importlib\n",
    "# importlib.reload(calc_token_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63a0cf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 422])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109e2603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = encodings\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride = 1\n",
    "seq_len = encodings.size(1)\n",
    "\n",
    "nlls = defaultdict(list)\n",
    "prev_end_loc = 0\n",
    "\n",
    "for begin_loc in range(0, seq_len, stride):\n",
    "    print(\"begin_loc: \", begin_loc)\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    print(\"end_loc: \", end_loc)\n",
    "    trg_len = end_loc - prev_end_loc\n",
    "    print(trg_len)\n",
    "    input_ids = encodings[:, begin_loc:end_loc].to(device_name)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "    target_token = target_ids[:, -stride]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        token_nll = outputs.loss * trg_len\n",
    "        \n",
    "        target_word = tokenizer.decode(target_token)\n",
    "        nlls[target_word].append(token_nll.cpu().item())\n",
    "        \n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d944d23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "inf\n",
      " after\n",
      "2677.2778\n",
      " my\n",
      "6387.746\n",
      " roommate\n",
      "675372.3\n",
      " came\n",
      "981.5949\n",
      " home\n",
      "12283.591\n",
      " :-)\n",
      "13612.111\n",
      " :\n",
      "12652.543\n",
      " https\n",
      "1632.5735\n",
      "://\n",
      "1.000114\n",
      "i\n",
      "2282.7717\n",
      ".\n",
      "63.961105\n",
      "imgur\n",
      "107.21448\n",
      "com\n",
      "35.206013\n",
      "/\n",
      "767.95355\n",
      "Due\n",
      "56668544.0\n",
      "uy\n",
      "23994.855\n",
      "gu\n",
      "10058.086\n",
      "jpg\n",
      "260.2855\n",
      "\n",
      "\n",
      "46.925888\n",
      "Tu\n",
      "139882.14\n",
      " ded\n",
      "2828744.0\n",
      "ic\n",
      "1188.3696\n",
      "aci\n",
      "36138.34\n",
      "ón\n",
      "1.0501997\n",
      " es\n",
      "78149.99\n",
      " real\n",
      "3874.089\n",
      "ment\n",
      "324.4796\n",
      "e\n",
      "12871.589\n",
      " admirable\n",
      "98058370.0\n",
      "Des\n",
      "8996.796\n",
      "af\n",
      "3523.3965\n",
      "ortun\n",
      "13955.19\n",
      "ad\n",
      "266449.25\n",
      "ament\n",
      "10577.184\n",
      " la\n",
      "7698.388\n",
      " com\n",
      "21651.275\n",
      "un\n",
      "12717.073\n",
      "idad\n",
      "1.7455468\n",
      " or\n",
      "668.5202\n",
      "t\n",
      "502226.75\n",
      "odox\n",
      "29027.328\n",
      "a\n",
      "947.4002\n",
      " en\n",
      "39403.74\n",
      " Pan\n",
      "310433.47\n",
      "am\n",
      "14040.1875\n",
      "á\n",
      "195376.05\n",
      " no\n",
      "510.27524\n",
      " m\n",
      "3818.892\n",
      " \"\n",
      "414.83902\n",
      "fan\n",
      "636506.5\n",
      "\"\n",
      "299.68466\n",
      " de\n",
      "64793.91\n",
      " los\n",
      "4.3067017\n",
      " convers\n",
      "9238.641\n",
      "os\n",
      "2498.2039\n",
      " No\n",
      "9446.969\n",
      " con\n",
      "10489.393\n",
      "oz\n",
      "26433.963\n",
      "co\n",
      "728.42664\n",
      " n\n",
      "5722.8228\n",
      "ie\n",
      "11234.358\n",
      " que\n",
      "916563.94\n",
      " se\n",
      "11798.665\n",
      " h\n",
      "14394.142\n",
      "aya\n",
      "7188.74\n",
      " convert\n",
      "4686.5693\n",
      "ido\n",
      "684.82513\n",
      " Ad\n",
      "25356.38\n",
      "em\n",
      "4698.772\n",
      "ás\n",
      "166767.98\n",
      " las\n",
      "143.08153\n",
      " sin\n",
      "1150.7112\n",
      "agog\n",
      "480.4592\n",
      "as\n",
      "1818.1213\n",
      " nun\n",
      "12625.98\n",
      "ca\n",
      "77962.8\n",
      "jar\n",
      "1486503.0\n",
      "í\n",
      "89704.69\n",
      "an\n",
      "1272.9088\n",
      " ent\n",
      "65668.92\n",
      "rar\n",
      "129968.22\n",
      " person\n",
      "3337.9377\n",
      " pro\n",
      "7672.507\n",
      "ces\n",
      "16036.487\n",
      "o\n",
      "2815.1047\n",
      "...\n",
      "3807.4011\n",
      "Lo\n",
      "4512423.5\n",
      " me\n",
      "1350.5175\n",
      "j\n",
      "237643.31\n",
      "or\n",
      "1161.3945\n",
      " p\n",
      "7804.482\n",
      "ued\n",
      "246.50125\n",
      "es\n",
      "4333.9546\n",
      "acer\n",
      "258.76877\n",
      " ir\n",
      "37517.797\n",
      " a\n",
      "203.00772\n",
      "ag\n",
      "53754.465\n",
      "oga\n",
      "11899790.0\n",
      " reform\n",
      "6564.8447\n",
      "ista\n",
      "3005.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, value in nlls.items():\n",
    "    print(key)\n",
    "    ppl = torch.exp(torch.stack(value)).mean()\n",
    "    print(ppl.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
